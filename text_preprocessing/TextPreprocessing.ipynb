{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Y\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Y\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "## IMPORTS\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer #import WordNetLemmatizer\n",
    "#nltk.download WordNetLemmatizer()\n",
    "import re\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0   \"The mother likes eating scones for breakfast...\n",
      "1  I am happy to see you, it has been forever, wo...\n",
      "2                               Catch me if you can!\n"
     ]
    }
   ],
   "source": [
    "## CREATE DATA PD\n",
    "\n",
    "pilot = [' \"The mother likes eating scones for breakfast$ on rainy days.\"','I am happy to see you, it has been forever, would you like to grab a coffee sometime **?','Catch me if you can!']\n",
    "pilot2 = pd.DataFrame(pilot)\n",
    "print (pilot2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \"The mother likes eating scones for breakfast$ on rainy days.\"'\n",
      " 'I am happy to see you, it has been forever, would you like to grab a coffee sometime **?'\n",
      " 'Catch me if you can!']\n"
     ]
    }
   ],
   "source": [
    "## NUMPY\n",
    "pilot3 = np.asarray(pilot)\n",
    "print (pilot3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION TO CLEAN REGULAR EXPRESSIONS USING PANDA\n",
    "def preprocess(sentences):\n",
    "    new_data = []\n",
    "    for sentence in (sentences[:][0]):\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' the mother likes eating scones for breakfast on rainy days', 'i am happy to see you it has been forever would you like to grab a coffee sometime ', 'catch me if you can']\n"
     ]
    }
   ],
   "source": [
    "clean = preprocess(pilot2)\n",
    "print (clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION TO CLEAN REGULAR EXPRESSIONS USING PANDA\n",
    "def preprocessNUM(sentences):\n",
    "    new_data = []\n",
    "    for sentence in (sentences[:]):\n",
    "        new_sentence = re.sub('<.*?>', '', sentence) # remove HTML tags\n",
    "        new_sentence = re.sub(r'[^\\w\\s]', '', new_sentence) # remove punctuation\n",
    "        new_sentence = new_sentence.lower() # convert to lower case\n",
    "        if new_sentence != '':\n",
    "            new_data.append(new_sentence)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' the mother likes eating scones for breakfast on rainy days', 'i am happy to see you it has been forever would you like to grab a coffee sometime ', 'catch me if you can']\n"
     ]
    }
   ],
   "source": [
    "clean = preprocessNUM(pilot3)\n",
    "print (clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world!', 'I am going for coffee before work!']\n"
     ]
    }
   ],
   "source": [
    "## SENTENCES TOKENIZATION\n",
    "paragraph = 'Hello world! I am going for coffee before work!'\n",
    "sent_text = sent_tokenize(paragraph)\n",
    "print(sent_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'I', 'am', 'going', 'for', 'coffee', 'before', 'work', '!']\n"
     ]
    }
   ],
   "source": [
    "## WORD TOKENIZATION\n",
    "words = word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      0\n",
      "0   Hello world. I am happy to see you!\n",
      "1  Greta lost her bag and shoes, again.\n",
      "2                    extra no thank you\n",
      "['Hello world. I am happy to see you!'\n",
      " 'Greta lost her bag and shoes, again.' 'extra no thank you']\n"
     ]
    }
   ],
   "source": [
    "## TEST PRINT - NUMPY and PANDAS\n",
    "pilot4 = ['Hello world. I am happy to see you!', 'Greta lost her bag and shoes, again.','extra no thank you']\n",
    "pilot5 = pd.DataFrame(pilot4)\n",
    "pilot6 = np.asarray(pilot4)\n",
    "print(pilot5)\n",
    "print(pilot6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SENTENCE TOKENIZATION FUNCTION PANDAS\n",
    "def tokenization_s_pd(sentences):\n",
    "    sent_new = []\n",
    "    for sent in (sentences[:][0]):\n",
    "        sent_tok = sent_tokenize(sent)\n",
    "        if sent_tok != '':\n",
    "            sent_new.append(sent_tok)\n",
    "    return sent_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello world.', 'I am happy to see you!'], ['Greta lost her bag and shoes, again.'], ['extra no thank you']]\n"
     ]
    }
   ],
   "source": [
    "test = tokenization_s_pd(pilot5)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PANDA WORK TOKENIZATION\n",
    "def wordtok(wordlist):\n",
    "    new_words = []\n",
    "    for words in (wordlist):\n",
    "        work_tok = word_tokenize(words)\n",
    "        new_words.append(work_tok)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SENTENCE TOKENISATION FUNCTION NUMPY\n",
    "def tokenization_s_np(sentences):\n",
    "    sent_new = []\n",
    "    for sent in (sentences[:]):\n",
    "        sent_tok = word_tokenize(sent)\n",
    "        if sent_tok != '':\n",
    "            sent_new.append(sent_tok)\n",
    "    return sent_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', 'world', '.', 'I', 'am', 'happy', 'to', 'see', 'you', '!'], ['Greta', 'lost', 'her', 'bag', 'and', 'shoes', ',', 'again', '.'], ['extra', 'no', 'thank', 'you']]\n"
     ]
    }
   ],
   "source": [
    "test2 = tokenization_s_np(pilot6)\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "snowball = SnowballStemmer(language = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:][0])]\n",
    "    new.append(stem_words)\n",
    "    return new       #  ' '.join(stem_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem1212(words):\n",
    "    new = []\n",
    "    for x in (words[:][0]):\n",
    "        stem_words = snowball.stem(x)\n",
    "        \n",
    "        \n",
    "    return stem_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(words):\n",
    "    new = []\n",
    "    lem_words = [lemmatizer.lemmatize(x) for x in (words[:][0])]\n",
    "    new.append(lem_words)\n",
    "    return new  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'like', 'to', 'eat', 'apple', 'he', 'ha', 'eaten', 'many', 'apple', 'because', 'he', 'like', 'eating']]\n"
     ]
    }
   ],
   "source": [
    "testing = ['You like to eat apples. He has eaten many apples because he likes eating.']\n",
    "tt = pd.DataFrame(testing) # convert to pandas\n",
    "tt2 = np.asarray(tt)\n",
    "yolo = preprocess(tt) #preprocess\n",
    "yolito = wordtok(yolo) #chop up into words\n",
    "stemtest = stemming(yolito)\n",
    "\n",
    "sentest = tokenization_s_pd(tt)\n",
    "lemtest = lemmatization(yolito)\n",
    "\n",
    "print(lemtest)\n",
    "#print(tt2)\n",
    "#print(sentest)\n",
    "#print(tt)\n",
    "#print(stemtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
